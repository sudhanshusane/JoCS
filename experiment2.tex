%\input{combined_vis.tex}
%\input{timings.tex}

This section provides an overview of our experiments.
%
It is organized as follows: 
experiments performed (\ref{sec:experiments}), simulation codes~(\ref{sec:datasets}),
software implementation (\ref{sec:infra}),
hardware (\ref{sec:runtime}),
and metrics (\ref{sec:metrics}).

\subsection{Experiment Overview}
\label{sec:experiments}

Our experiments are designed in response to the evaluation ``areas''
from Section~\ref{sec:instantiation}.
%
Since our evaluations can be separated into two distinct phases, 
we organized our experiments into two distinct campaigns (one for
\textit{in situ} encumbrance and one \textit{post hoc} efficacy),
although some of the
experiments were used in both campaigns.
%

Our experiments considered five basic factors:
\begin{tightItemize}
\item Simulation code
\item Number of particles (Lagrangian basis flows)
\item Interval (number of cycles between saves)
\item Grid size
\item Concurrency
\end{tightItemize}
\noindent
For each factor, there are many possible options.
%
Therefore, running experiments for the cross-product of options
was prohibitive, especially since we had limited time on our supercomputer
(1000 node hours).
%
Instead, we sampled the space of possible options.
%
For both campaigns, our organization was around our three
simulation codes: Cloverleaf3D, SW4, and Nyx 
(described in subsection~\ref{sec:datasets}).
%
For a given simulation code, we varied some factors and fixed others.
%
Our goal was to simultaneously
 provide coverage and yet allow us to see the impact of certain factors,
all while staying within our compute budget.
%
In all, we ran 47 experiments, 22 for \textit{in situ} encumbrance and 25
for \textit{post hoc} efficacy.
%
Table~\ref{tab:campaign1} shows our choices for the \textit{in situ}
encumbrance campaign, while Table~\ref{tab:campaign2} shows
our choices for the \textit{post hoc} efficacy campaign.
%
Specific choices for options 
%(e.g., how many Lagrangian basis flows are used for Cloverleaf3D?) 
are documented in the Results section.

\begin{table}[h]
\centering
\vspace{-2mm}
\scalebox{0.9}{
\begin{tabular}{|r||c|c|c|c|c|}
\hline
Simulation Code & Cloverleaf3D & SW4 & Nyx \\ \hline
\# of Particles  &   3 & 3 & 3 \\
Interval &  3 & 1 & 1 \\
Grid Size & 1 & 3,2 & 2 \\
Concurrency & 1 & 2 & 1 \\  \hline
Total Experiments & 9 & 7 & 6 \\ \hline
\end{tabular}
}
\vspace{-3mm}
\caption{\label{tab:campaign1}Experimental overview for the \textit{in situ} encumbrance campaign.  For SW4, we were able to run a very fine grid size at low concurrency, but not the entire cross product of options due to limitations in compute time.  Overall, we considered 22 experiments for this campaign.}
\end{table}

\begin{table}[h]
\centering
\scalebox{0.9}{
\begin{tabular}{|r||c|c|c|c|c|}
\hline
Simulation Code                  & Cloverleaf3D & SW4 & Nyx \\ \hline
\# of Particles  &   3 & 4 & 3 \\
Interval &  3 & 1 & 4 \\
Grid Size & 1 & 1 & 1 \\
Concurrency & 1 & 1 & 1 \\  \hline
Total Experiments & 9 & 4 & 12 \\ \hline
\end{tabular}
}
\vspace{-3mm}
\caption{\label{tab:campaign2}Experimental overview for the post hoc efficacy campaign.
%
Overall, we considered 25 experiments for this campaign.}
\end{table}


%The remainder of this subsection describes the simulation codes we study
%(\ref{sec:datasets}) and the motivation for studying the other factors (\ref{sec:parameters}).

\subsection{Simulation Codes}
\label{sec:datasets}
For our study we consider three simulation application codes that are used and/or developed as part of the 
Exascale Computing Project from the
United States Department of Energy.
%

First, we use the Cloverleaf3D~\cite{mallinson2013cloverleaf} mini or proxy ECP application that solves compressible Euler equations in a hydrodynamics setting on a Cartesian grid using an explicit second-order method. 
%
Cloverleaf3D has been developed and used by several studies to evaluate emerging architectures and various techniques targeting Exascale applications.
%
The simulation is initially relatively stable and begins with an energy bar expanding from the center of the XY plane along the Z-axis. 
%
Figure~\ref{fig:teaser}a show pathlines calculated in the Cloverleaf3D domain that show this initial behavior in the simulation.

Next, we consider the SW4 seisomology simulation~\cite{petersson2015wave}.
%
This is an ECP application developed to study seismic wave propagation.
%
It operates and produces multiple domains with a time-dependent displacement field depending on the input deck provided to it.
%
We operate on a single domain and use the displacement vector field as input to our \textit{in situ} Lagrangian operator.
%
Figure~\ref{fig:teaser}b is generated by visualizing the displacement magnitude of the particle trajectories extracted over the first 1000 cycles.
%
%\fix{remove these sentences?:}
%The visualization uses line contours, with each node selecting 10 isovalues for the range of displacement magnitude in the node.
%
%The color map range is the same for the entire domain, i.e., all nodes use the same maximum and minimum values.
%

The last data set we consider is the Nyx cosmology simulation~\cite{almgren2013nyx}, another ECP application.
%
The simulation's hydrodynamics is based on a compressible flow formulation in Eulerian coordinates. 
%
We built an Lya executable used to model Lyman-alpha forest in quasar spectra.
%
For this simulation, we derived the velocity field using the fields of momentum and density.
%
%\fix{Too long!}
%Figure~\ref{fig:vectorfield_nyx} shows a slice of the Nyx vector field at two time slices.
%
%We observed that the unit vectors at each grid point remain relatively the same across all cycles.
%
%The evolution of the vector field is in terms of velocity magnitude.
%
%The maximum velocity magnitude in the domain increases steadily for every cycle of the application we simulated.
%
%Further, Figure~\ref{fig:pathlines_nyx} is a visualization of the volume of the domain using 100,000 randomly seeded pathlines integrated for the first 25 cycles of the simulation.
%
%\input{vectorfield_nyx.tex}


%\subsubsection{Parameter Space}
%\label{sec:parameters}

%We vary the different ``knobs'' in the parameter space to evaluate the viability and efficacy of using in situ Lagrangian analysis to perform exploratory time-dependent vector field analysis and visualization. 
%
%Our study considers the following parameters:
%\begin{itemize}
%\item \textbf{Number of particles:} Our study varies the number of particles initialized per node and thus inform the cost of performing particle advection for varying workloads every cycle of the simulation. Further, the number of particles initialized is directly impacts the size of the data stored to disk and the accuracy of the reconstruction.
%%
%We specify the number of particles initialized using the notation \textbf{1:X}, where X is the reduction factor. 
%%
%For example, a 1:1 configuration states that one particle is used for every grid point (no reduction) and a 1:8 configuration states that one particle is used for every 8 grid points (12.5\% of the original data size). 
%%
%\item \textbf{Interval:} We consider the interval or frequency at which files are stored to disk. 
%%
%For a given total number of simulation cycles, this impacts the total amount of data stored to disk. 
%%
%Additionally, for the Lagrangian representation, the interval is directly related to the integration length of each massless particle. 
%%
%%For each configuration, we specify the number of cycles between storing to disk and refer to this as the \textbf{interval}. 
%%
%\item \textbf{Grid size:} We consider different grid sizes to measure the in situ encumbrance of varying workloads. 
%%
%Different grid sizes will use a different number of particles to sample the domain reasonably accurately.
%%
%In particular, we are interested in the in situ encumbrance when a single compute node is operating on a large number of grid points.
%
%\item \textbf{Concurrency:} We consider the costs at various scale (i.e., number of compute nodes, MPI ranks). Further, the simulation codes required different parallelization hardware and thus, across simulation codes we measure the costs of Lagrangian representation extraction using, both, GPUs and CPUs for particle advection.
%\end{itemize}

\subsection{Software Implementation}
\label{sec:infra}
\subsubsection{In Situ Reduction}
\label{sec:insituimp}
\input{insitu.tex}
\subsubsection{Post Hoc Exploration}
\input{posthoc.tex}

%We ran with VTK-m in Ascent.

\subsection{Runtime Environment}
\label{sec:runtime}
Our empirical study uses the Summit supercomputer at ORNL. 
%
A Summit compute node has two IBM Power9 CPUs, each with 21 cores running at 3.8 GHz and 512 GBytes of DDR4 memory. 
%
Nodes on Summit also have enhanced on-chip acceleration with each CPU connected via NVLink to 3 GPUs, for a total of 6 GPUs per node. 
%
Each GPU is an NVIDIA Tesla V100 with 5120 CUDA cores, 6.1 TeraFLOPS of double precision performance, and 16 GBytes of HBM2 memory.
%
Lastly, it uses a Mellanox EDR 100G InfiniBand, Non-blocking Fat Tree as its interconnect topology.

%Alaska is the small cluster at our research lab.
%%
%The head node with consists of two Intel Xeon E5-2667v3 CPU, each with 16 cores running at 3.2 GHz. 
%
%We use Summit to build, integrate, and execute in situ Lagrangian analysis with simulation codes.
%%
%Thus, all of our in situ extraction is performed on Summit. 
%%
%Additionally, we use Summit to perform large distributed memory post hoc reconstructions.
%%
%We use Alaska for the smaller post hoc reconstruction processes.



%Most of our experiment test configurations have two phases: in situ and post hoc. 
%
%The performance characteristics of the in situ phase is what we are primarily interested in.
%
%We execute extra ``only-extraction'' configurations to gauge in situ performance and that do not continue to the post hoc phase.
%
%The post hoc phase in our study is used to measure accuracy.
%
%We report timings associated with performing post hoc Lagrangian reconstruction on a single node and in a distributed memory setting.
%
%We strongly believe that as in situ Lagrangian representations evolve, the post hoc techniques used will as well.
%
%Currently, to the best of our knowledge there does not exist a perfect one-size-fits-all post hoc Lagrangian-based advection method.

%\input{sw4_vis.tex}
%\input{pathlines_clover.tex}
%\input{pathlines_nyx.tex}

\subsection{Evaluation Metrics}
\label{sec:metrics}

\subsubsection{In Situ Encumbrance}
\label{sec:encumbrance}
%
Our empirical study measures \textit{in situ} encumbrance in terms of execution time and memory usage.
%
For \textbf{ISR-1}, we measure the cost of each invocation of the Lagrangian VTK-m filter and report the average time, i.e., cost of particle advection for one cycle or \textbf{Step}.
%.
Additionally, we measure and report the percentage of simulation time spent on data analysis and visualization routines, or \textbf{DAV\%}.
%
For this we consider the simulation cycle time or Sim$_{cycle}$.
For \textbf{ISR-2}, we measure the runtime memory cost incurred by every compute node to maintain the state (current position) of particles at runtime in Bytes.


\subsubsection{I/O Cost}
\label{sec:iocost}
In this empirical study, we do not report or factor the cost of I/O write times. 
%
Besides being an operation that is performed infrequently in our case, 
we observed for the scale of study we conducted that Summit provides very 
fast write times.
%
Table~\ref{table_binary} documents write times of varying file sizes in binary format on Summit.
%
\input{table_binary.tex}
Given the range of file sizes stored to disk by a single MPI rank in our empirical study is between 0.5 MB to 115 MB, we believe this cost is negligible at the scale that we test (and perhaps, even at larger scales).
%
Thus, we limit our measurement and discussion of I/O to the total data storage required on the file system and report \textbf{DS-1} in Bytes stored.

%Of course, I/O costs might be relevant if the operation was frequent at full scale --- all the nodes with several MPI ranks each trying to write at the same time. 
%
%However, 

%We discuss our reasoning for this below. 
%
%When writing to a binary format we observed very fast write times on Summit. 
%%
%Summit is designed to provide low access latency and high bandwidth. 
%%
%We found that a binary file of size 200 MB can be written in 0.0231 seconds on a single compute node running a single MPI task. 
%%
%And it takes 0.171 seconds of wall time for 6 MPI ranks to each write 200 MB files in parallel. 
%%
%In our experiments the largest file stored to disk by a single rank for a single interval or cycle is 115 MB. 
%%
%The range of sizes varies from 0.5 MB to 115 MB per node per instance of storing to disk (0.002 to 0.04 seconds, respectively). 
%%
%We provide some of these timings for reference in Table~\ref{table_binary}.
%%
%Further, this infrequent operation and small cost of I/O in comparison 
%to the cost of a particle advection step which is performed each cycle is small.

%Although write times may be impacted at full scale --- all the nodes with several MPI ranks each trying to write at the same time - a study of this scale is beyond the scope of this work and more oriented toward a supercomputer I/O performance study. 
%
%Further, in our relatively small study to inform ourselves, we found noise that we attribute to the cluster running hundreds of jobs simultaneously to be significant. 

%Further, since \textbf{L-ISR} in intended to be used as a data reduction operator, the sizes of files written to disk would be approximately the same size (1:1) or significantly smaller (1:8, 1:27, 1:64). 
%
%If I/O costs were to be impacted, either due to scale or file format type, then the cost of Eulerian would be more adversely affected if compared against a reduced Lagrangian I/O cost. 
%

%Another point to consider is the infrastructure performing the operation of storing data. 
%
%Depending on whether the data is being written to a burst buffer or some staging platform before moving data to permanent storage can significantly alter the value of these timings. 

%All this being said, at scale scientists will not choose to save all the data due to excessive storage costs and since this would then indeed introduce a large cumulative encumbrance. 
%
%Instead, they resort to temporal subsampling, i.e., saving data out at a predetermined frequency or at select cycles. 
%
%One of our key assumptions is that as data sizes get larger, scientists will be forced to save less frequently or to store reduced data sets. 





\subsubsection{Post Hoc Efficacy}
\label{sec:evaluation}
Our empirical study measures time-dependent vector field reconstruction error by evaluating the accuracy of test particles trajectories interpolated using the extracted Lagrangian representation.
%
%For our Cloverleaf3D and Nyx experiments, we randomly place 100,000 and 50,000 test particles in the domain respectively.
%
%For the SW4 experiments, we place 90,000 test particles randomly in between $Z=5000$ and $Z=15,000$ (layer of activity in the domain).
%

For \textbf{PHE-1}, our empirical study measures the L2-norm, i.e., the Euclidean distance, for each interpolated point and compares it to a ground truth that is precomputed using the complete simulation data. 
%
We use \textbf{Avg$_{L2}$} and \textbf{Max$_{L2}$} to denote the average and maximum L2-norm for an individual particle trajectory, respectively.
%
%We note, the Max$_{L2}$ was similar to the end point distance between the ground truth and test particle trajectories.
%

An overall average of \textbf{Avg$_{L2}$}, denoted by \textbf{AvgN$_{L2}$}, is measured across $N$ test particle samples and provides a robust statistic~\cite{agranovsky2014improved, sane2018revisiting, sane2019interpolation, rapp2019void}.
%
Unlike \textbf{AvgN$_{L2}$}, a maximum error is more susceptible to outliers that could arise from small but complex regions of flow.
%
To provide a more detailed quantitative analysis compared to prior work, we use histograms to capture the distribution of error~(\textbf{Avg$_{L2}$}, \textbf{Max$_{L2}$}) across all test particles and provides insight into per particle outcomes.  
%
Further, for \textbf{PHE-2} we include a study of \textit{post hoc} reconstruction costs. 

%\input{accuracy.tex}
